<h4 align="center"> 
	üöß  Estudos em Aprendizagem por Refor√ßo üöÄ Em constru√ß√£o...  üöß
</h4>

# Estudos Aprendizagem por Refor√ßo
<p align="center">
 <a href="#introducao">Introdu√ß√£o</a> ‚Ä¢
 <a href="#motivacao">Motiva√ß√£o</a> ‚Ä¢ 
 <a href="#rl">Entendendo RL</a> ‚Ä¢ 
 <a href="#markov">Decis√£o de Markov</a> ‚Ä¢ 
 <a href="#bellman">Equa√ß√£o de Bellman</a> ‚Ä¢ 
 <a href="#industria">Ind√∫stria</a>  ‚Ä¢ 
 <a href="#exemplo">Exemplo Pr√°tico</a> ‚Ä¢ 
</p>

## Introdu√ß√£o
<p align="justify">Para compreender a import√¢ncia do Aprendizado por Refor√ßo (Reinforcement Learning - RL) , √© importante salientar que o RL √© conhecido como modelo de aprendizado semi-supervisionado em Aprendizado de M√°quina (Machine Learning), m√©todo inserido no campo da Intelig√™ncia Artificial, isto √©, o estudo em que m√°quinas s√£o utilizadas com o prop√≥sito de apreenderem e raciocinarem o mais pr√≥ximo poss√≠vel da mente humana. Mas, afinal, o que √© intelig√™ncia? O conceito pode ser definido como o conjunto de compet√™ncias caracterizado por: adapta√ß√£o, reconhecimento, aprendizado, interpreta√ß√£o, tomada de decis√£o e racioc√≠nio. Tudo isso, ser√° de suma import√¢ncia no entendimento e aplica√ß√£o do Aprendizado por refor√ßo. No universo da rob√≥tica, a import√¢ncia do RL deve-se ao fato da possibilidade de estrat√©gia, movimenta√ß√£o e controle e, de modo ascendente, suas aplica√ß√µes e m√©todos de uso t√™m criado ra√≠zes e grandes resultados ao meio acad√™mico, cient√≠fico, industrial, comercial e educacional.</p>

## Motiva√ß√£o
<p align="justify">O ramo da Intelig√™ncia Artificial, al√©m de amplo e relativamente recente, possui uma certa complexidade e exige o entendimento pr√©vio de diversos conceitos computacionais e t√©cnicas de programa√ß√£o. Com isso em mente, m√∫ltiplas Empresas e Institui√ß√µes foram criadas a fim de tornar o aprendizado de AI atrativo e din√¢mico. Neste t√≥pico, o foco ser√° em uma Institui√ß√£o espec√≠fica: a OpenAI.</p>

<p align="justify">A OpenAI √© uma Institui√ß√£o desprovida de fins lucrativos voltada √† pesquisa de Intelig√™ncia Artificial. Seu foco √© fornecer o desenvolvimento de IA de forma amig√°vel e, ainda, possui como miss√£o beneficiar a sociedade como um todo. Abaixo, ser√£o apresentados 2 exemplos de destaque desenvolvidos pela equipe da OpenAI. 
	O primeiro projeto √© a resolu√ß√£o do cubo de Rubik utilizando-se uma m√£o rob√≥tica.</p>

| Rubik's Cube |
| ------------ |
| <img src="https://cdn.openai.com/solving-rubiks-cube/images/dr.jpg" width="500">|
| Fonte: Fonte: OpenAI.
Dispon√≠vel em < https://openai.com/blog/solving-rubiks-cube/ >. Acesso em: 27 fev. 2021. |

<p align="justify">De certo, resolver o cubo de Rubik j√° n√£o √© uma tarefa f√°cil nem para um humano, o qual apenas √© capaz de resolv√™-lo devido sua capacidade de manipular objetos e operar com as m√£os, resultado de milhares de anos de evolu√ß√£o. Com isso em mente, a OpenAI enfrentou o desafio de, al√©m de desenvolver um algoritmo capaz de aprender a executar tal tarefa, construir uma m√£o rob√≥tica o mais pr√≥ximo poss√≠vel da m√£o humana. A abordagem utilizada consistiu em centenas de ambientes de simula√ß√£o que possibilitaram a aprendizagem da m√£o rob√≥tica a realizar as tarefas desejadas, n√£o apenas no mundo virtual, mas tamb√©m no mundo real. No contexto computacional, mais especificamente no que se refere ao algoritmo de aprendizado por refor√ßo do projeto, a t√©cnica estabelecida para o processo foi a ‚ÄúRandomiza√ß√£o autom√°tica de dom√≠nio‚Äù - t√©cnica j√° desenvolvida pela pr√≥pria OpenAI - a qual fundamenta-se na cria√ß√£o de ambientes aleat√≥rios com complexidades diferenciadas e cada vez mais complexas, de forma que a m√£o rob√≥tica venha a lidar com situa√ß√µes nunca vistas antes. Nesse sentido, a m√£o foi testada em diversos cen√°rios adversos para constata√ß√£o de sua capacidade e habilidade, tais como: presen√ßa de vento, utiliza√ß√£o de luvas, interfer√™ncia por objetos, testes de elasticidade, fric√ß√£o, tamanho e massa vari√°vel do cubo. Apesar de ainda n√£o ser capaz de resolver o cubo 100% das vezes, √© esperado que , em breve,  o conjunto m√£o rob√≥tica e algoritmo de aprendizagem por refor√ßo sejam totalmente eficientes nos mais variados ambientes e complexas situa√ß√µes.</p>

## Entendendo RL
<p align="justify">Para uma compreens√£o geral do funcionamento de um algoritmo de RL, √© necess√°rio entender os principais conceitos que o envolvem. Entre eles, t√™m-se o agente, a recompensa, a a√ß√£o, o ambiente e o estado. Basicamente, a rela√ß√£o pr√°tica e te√≥rica de todos esses conceitos leva ao objetivo final do RL: fazer um objeto (agente), seja ele f√≠sico ou virtual aprender a fazer algo (tarefas humanas). O agente aprende por meio da intera√ß√£o com o ambiente, sem um conhecimento pr√©vio do que deve realizar. As recompensas (r:(S √ó A) ‚Üí R) , as quais podem ser  positivas ou negativas, s√£o ‚Äúfeedbacks‚Äù do ambiente sobre o comportamento do agente. A ideia geral de um algoritmo de RL √© fazer com que essas recompensas sejam maximizadas, ou seja, quanto maior for a recompensa de determinada a√ß√£o, melhor a a√ß√£o tomada √© para o aprendizado. J√° o ambiente pode ser um ambiente real ou um ambiente de simula√ß√£o. O agente deve estar preparado para um ambiente de constantes mudan√ßas, n√£o para um ambiente estagnado.</p>

>‚Äú _Aprendizado por refor√ßo √© uma abordagem computacional para entender e automatizar o aprendizado direcionado a objetivos e tomada de decis√£o. √â distinguido de outras abordagens computacionais pela √™nfase no aprendizado de um agente pela intera√ß√£o direta com o ambiente, sem depender de uma supervis√£o exemplar ou completar modelos de ambientes_‚Äù (Richard S. Sutton and Andrew G. Barto, 2015, p.29).

![Aprendizado por refor√ßo](https://github.com/glahr/estudos_aprendizagem_por_reforco/blob/main/aprendizado%20por%20reforco.png?raw=true)

#### Simbologia:

:small_orange_diamond: s ‚Üí estado<br />
:small_orange_diamond: a ‚Üí  a√ß√£o<br />
:small_orange_diamond: S ‚Üí conjunto de todos os estados n√£o terminais<br />
:small_orange_diamond: S+ ‚Üí  conjunto de todos os estados, incluindo o estado terminal<br />
:small_orange_diamond: A(s) ‚Üí  conjunto de a√ß√µes para o estado s<br />
:small_orange_diamond: R ‚Üí  conjunto de poss√≠veis recompensas<br />
:small_orange_diamond: ùë¶ ‚Üí desconto<br />

<p align="justify">Durante o  funcionamento de um algoritmo de Reinforcement Learning,  o agente encontra-se em um certo estado. Em seguida, realiza uma determinada a√ß√£o, passando a outro estado. Por fim, recebe uma recompensa. O objetivo √© maximizar o conjunto de tais recompensas, para que o agente tenha resultados melhores e mais pr√≥ximos o poss√≠vel do esperado. Pode-se dizer que, a princ√≠pio, os testes realizados com o agente devem ‚Äúfalhar‚Äù, com o prop√≥sito de que o agente identifique o que n√£o se deve fazer e, assim, obter resultados futuros mais precisos e eficientes. Basicamente, o agente ir√° aprender com seus erros e acertos, assim como um ser humano durante toda a sua vida. Uma equa√ß√£o que podemos utilizar para lidar com a modelagem em aprendizado por refor√ßo √© a chamada equa√ß√£o de Bellman, vamos verificar na pr√≥xima se√ß√£o.</p>

## Processo de decis√£o de Markov e Equa√ß√£o de Bellman

<p align="justify">Em Aprendizado por Refor√ßo, o processo que normalmente sustenta a aplica√ß√£o √© o Processo de decis√£o de Markov (Markov decision process - MDP), o qual √© caracterizado pela estocasticidade. O processo consiste em um conjunto de estados, a√ß√µes, modelo de transi√ß√£o e recompensas, representando por: <T,S,,A,R(s)>, em que T - P(s‚Äô| s,a) - √© a probabilidade de dado agente mover para o estado s‚Äô de s executando determinada a√ß√£o a. Para auxiliar tal processo de decis√£o, √© utilizada a equa√ß√£o de Bellman.
Contextualizando, Richard Ernest Bellman foi um grande contribuidor no ramo da matem√°tica e da computa√ß√£o devido √† inven√ß√£o da programa√ß√£o din√¢mica. Sua equa√ß√£o tem grande import√¢ncia, no sentido de otimiza√ß√£o de valores em uma fun√ß√£o.
A seguir, temos a equa√ß√£o (1) - representa a rela√ß√£o de um valor de estado e dos valores de estados sucessores - e a equa√ß√£o (2), em que √© calculado o valor representante de tomar determinada a√ß√£o a em um estado s, sob uma pol√≠tica pi : </p>


<img src="https://render.githubusercontent.com/render/math?math=Q(s,a) = Q(s,a) + \alpha [R(s',a)+\gamma max_{a'} Q'(s',a') - Q(s,a)]">



### Pr√©-requisitos
<p align="justify">Antes de come√ßar, voc√™ vai precisar ter instalado em sua m√°quina as seguintes...</p>





```python
import numpy as np 
```

